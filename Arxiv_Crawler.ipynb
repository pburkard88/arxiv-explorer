{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Crawler\n",
    "This contains tools for retrieving and parsing both papers and metadata from arXiv.org using Tor for anonymous web requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "import re\n",
    "import string\n",
    "import fake_useragent\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from TorCtl import TorCtl\n",
    "import stem\n",
    "import stem.connection\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "from __future__ import print_function, division, generators, unicode_literals, with_statement\n",
    "import os\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "references_pattern = re.compile('(^|\\n)R\\n?eferences\\n', re.IGNORECASE)\n",
    "# arxiv_citation_pattern = re.compile(\"arXiv:[0-9\\.]+\")\n",
    "arxiv_citation_pattern = re.compile(\"(arxiv.org\\/((pdf)|(abs))\\/(([a-z-]+\\/)?[0-9\\.]+))|(arXiv(.org)?:(([a-z-]+\\/)?[0-9\\.]+))\", re.IGNORECASE)\n",
    "subject_pattern = re.compile('\\(cs.[A-Z][A-Z]\\)')\n",
    "non_alpha_chars = string.punctuation + string.whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Tor password:········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "tor_password = getpass('Enter Tor password:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_ip(tor_password=None, controller_port=9051):\n",
    "    old_ip = request(\"http://icanhazip.com/\").text\n",
    "    renew_connection(tor_password)\n",
    "    new_ip = old_ip\n",
    "    try:\n",
    "        new_ip = request(\"http://icanhazip.com/\").text\n",
    "    except:\n",
    "        new_ip = old_ip\n",
    "    while new_ip == old_ip:\n",
    "        sleep(np.random.random())\n",
    "        renew_connection(tor_password)\n",
    "        try:\n",
    "            new_ip = request(\"http://icanhazip.com/\").text\n",
    "        except:\n",
    "            new_ip == old_ip\n",
    "            continue\n",
    "    return new_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def renew_connection(tor_password, controller_port=9051):\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        if tor_password:\n",
    "            controller.authenticate(password = tor_password)\n",
    "        else:\n",
    "            controller.authenticate()\n",
    "        controller.signal(Signal.NEWNYM)\n",
    "        controller.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# request a URL \n",
    "def request(url):\n",
    "    # communicate with TOR via a local proxy (privoxy)\n",
    "    ua = fake_useragent.UserAgent()\n",
    "    head = ua.random\n",
    "    proxy={'http': 'socks5://localhost:9050'}\n",
    "    return requests.get(url, proxies=proxy, headers={'User-Agent': head})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AuthenticationFailure",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationFailure\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-03ae13e13b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_new_ip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtor_password\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtor_password\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-15b981d6ccf8>\u001b[0m in \u001b[0;36mget_new_ip\u001b[0;34m(tor_password, controller_port)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_new_ip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtor_password\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontroller_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9051\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mold_ip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://icanhazip.com/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrenew_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtor_password\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnew_ip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_ip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-10b339176865>\u001b[0m in \u001b[0;36mrenew_connection\u001b[0;34m(tor_password, controller_port)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mController\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_port\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9051\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtor_password\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtor_password\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/stem/control.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/stem/connection.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(controller, password, chroot_path, protocolinfo_response)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mauth_exc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauth_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mauth_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m   \u001b[0;31m# We really, really shouldn't get here. It means that auth_exceptions is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAuthenticationFailure\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_new_ip(tor_password=tor_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Paper Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve citations in an arxiv PDF\n",
    "def extract_text_and_citations(pdf):\n",
    "    try:\n",
    "        num_pages = pdf.numPages\n",
    "    except:\n",
    "        return '', []\n",
    "    in_citations = False\n",
    "    text = []\n",
    "    citations = []\n",
    "    for page_num in range(num_pages):\n",
    "        page_text = ''\n",
    "        try:\n",
    "            page_text = pdf.getPage(page_num).extractText()\n",
    "#             print(page_text)\n",
    "        except:\n",
    "            print('Failed PDF parsing for document')\n",
    "            break\n",
    "        text.append(page_text)\n",
    "#         if page_num == 9:\n",
    "#             print(page_text)\n",
    "        if re.search(references_pattern, page_text):\n",
    "#             print(\"In Citations!\")\n",
    "            in_citations = True\n",
    "        if in_citations:\n",
    "#             print(page_num)\n",
    "            for citation in re.findall(arxiv_citation_pattern, page_text):\n",
    "                if citation[4] != '':\n",
    "                    citations.append(citation[4])\n",
    "                else:\n",
    "                    citations.append(citation[8])\n",
    "#             citations.extend(list(map(lambda x: x.strip('arXiv:' + non_alpha_chars), re.findall(arxiv_citation_pattern, page_text))))\n",
    "    return \"\\n\".join(text), citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse an arxiv PDF to find all details\n",
    "def parse_arxiv_paper(paper_metadata, pdf, pdf_dir='.', text_dir='.', citation_dir='.'):\n",
    "    # Can't have / in filename\n",
    "    legal_filename = paper_metadata['id'].replace('/', '_')\n",
    "    # Write the PDF to a local file\n",
    "    pdf_file_name = pdf_dir + '/' + legal_filename + '.pdf'\n",
    "    with open(pdf_file_name, 'wb') as f:\n",
    "        f.write(pdf)\n",
    "        \n",
    "    # Read the pdf file in for processing\n",
    "    pdf_reader = None\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfFileReader(pdf_file_name)\n",
    "    except:\n",
    "        print('Bad file for paper {}'.format(paper_metadata['id']))\n",
    "        return {'metadata': paper_metadata, 'citations': []}\n",
    "    text, citations = extract_text_and_citations(pdf_reader)\n",
    "    \n",
    "    # Write the text to a local file\n",
    "    with open(text_dir + '/' + legal_filename + '.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "        \n",
    "    # Write citations to a local file\n",
    "    with open(citation_dir + '/' + 'citations.txt', 'a') as f:\n",
    "        for citation in citations:\n",
    "            f.write(paper_metadata['id'] + ',' + citation + ',' + str(1) + '\\n')\n",
    "    return {'metadata': paper_metadata, 'citations': citations} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_arxiv_paper(arxiv_id, pdf_dir='.', text_dir='.', citation_dir='.', field=None):\n",
    "    sleep(0.5*np.random.random())\n",
    "    print('Retrieving Paper {}'.format(arxiv_id))\n",
    "    r = request('https://arxiv.org/pdf/{}.pdf'.format(arxiv_id))\n",
    "    print('Found Paper {}'.format(arxiv_id))\n",
    "    paper_metadata = {}\n",
    "    paper_metadata['id'] = arxiv_id\n",
    "\n",
    "    return parse_arxiv_paper(paper_metadata, r.content, pdf_dir, text_dir, citation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_papers_text(papers, get_text=False, pdf_dir='.', text_dir='.', citation_dir='.', field=None, batch_size=10, start_index=0, end_index=-1):\n",
    "    if end_index==-1:\n",
    "        end_index = len(papers)\n",
    "\n",
    "    # Make the citations directory if it doesn't exist\n",
    "    if not os.path.exists(citation_dir):\n",
    "        os.makedirs(citation_dir)\n",
    "\n",
    "    # Make the papers and text directories for this field if it doesn't exist\n",
    "    if field:\n",
    "        pdf_dir = pdf_dir + '/' + field\n",
    "        text_dir = text_dir + '/' + field\n",
    "        if not os.path.exists(text_dir):\n",
    "            os.makedirs(text_dir)\n",
    "        if not os.path.exists(pdf_dir):\n",
    "            os.makedirs(pdf_dir)\n",
    "    # Go thru batches of papers\n",
    "    for start in range(start_index, end_index)[0::batch_size]:\n",
    "        # Get minibatch to operate on with same IP\n",
    "        batch_ids = papers.iloc[start : start + batch_size].id\n",
    "        # Reset IP\n",
    "        print('Retrieved new IP {} for start {} of size {}'.format(get_new_ip(tor_password), start, batch_size))\n",
    "        for arxiv_id in batch_ids:\n",
    "            if os.path.exists(pdf_dir + '/' + arxiv_id.replace('/', '_') + '.pdf'):\n",
    "                print('ID skipped')\n",
    "                continue\n",
    "            while True:\n",
    "                try:\n",
    "                    paper = get_arxiv_paper(arxiv_id, pdf_dir=pdf_dir, text_dir=text_dir, citation_dir=citation_dir, field=field)\n",
    "#                     print(len(paper['citations']))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print('Request failed, got new IP {}'.format(get_new_ip(tor_password)))\n",
    "                    traceback.print_exc()\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ids_from_db(db_url, field):\n",
    "    conn = sqlite3.connect(db_url)\n",
    "    cur = conn.cursor()\n",
    "    res = cur.execute(\"SELECT * FROM Papers WHERE primary_subject = '\" + field + \"'\")\n",
    "    papers = res.fetchall()\n",
    "    return pd.DataFrame(papers, columns=['id', 'url', 'title', 'year', 'month', 'field', 'primary_subject', 'secondary_subject', 'tertiary_subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers = get_ids_from_db('arxiv_raw.sqlite', 'IR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_papers_text(papers, pdf_dir='Projects/ArXiv/data/papers/pdf', text_dir='Projects/ArXiv/data/papers/text', \n",
    "                citation_dir='Projects/ArXiv/data/citations', field='ir', start_index=1632, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_papers_metadata(soup, cur, field, year, month, log_file):\n",
    "    titles = soup.findAll('div', {'class': 'list-title'})\n",
    "    authors = soup.findAll('div', {'class': 'list-authors'})\n",
    "    paper_urls = soup.findAll('span', {'class': 'list-identifier'})\n",
    "    subjects = soup.findAll('div', {'class': 'list-subjects'})\n",
    "    if len(titles) != len(authors):\n",
    "        print('number of titles and authors mismatch')\n",
    "    else:\n",
    "        for title, author, paper_url, subject in zip(titles, authors, paper_urls, subjects):\n",
    "            arxiv_id = re.split(':', paper_url.find('a').text)[-1]\n",
    "            url = paper_url.find('a')['href']\n",
    "            title = title.contents[-1].strip()\n",
    "            paper_subjects = list(map(lambda x: x.strip('()cs.' + field.lower()), re.findall(subject_pattern, subject.text)))\n",
    "            secondary_subject = ''\n",
    "            tertiary_subject = ''\n",
    "            if (len(paper_subjects) > 1):\n",
    "                secondary_subject = paper_subjects[1]\n",
    "                if len(paper_subjects) > 2:\n",
    "                    tertiary_subject = paper_subjects[2]\n",
    "            try:\n",
    "                cur.execute('''\n",
    "                    INSERT OR IGNORE INTO Papers (id, url, title, year, month, category, primary_subject, secondary_subject, tertiary_subject) \n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''', (arxiv_id, url, title, int(year), int(month), 'CS', paper_subjects[0], secondary_subject, tertiary_subject))\n",
    "\n",
    "                cur.execute('SELECT id FROM Papers WHERE title = ? ', (title, ))\n",
    "#                 print(title)\n",
    "                paper_id = cur.fetchone()[0]\n",
    "\n",
    "                paper_authors = [au.string.strip() for au in author.findAll('a')]\n",
    "                for name in paper_authors:\n",
    "                    cur.execute('''\n",
    "                        INSERT OR IGNORE INTO Authors (name) \n",
    "                        VALUES (?)''', (name, ))\n",
    "                    cur.execute('SELECT id FROM Authors WHERE name = ? ', (name, ))\n",
    "                    author_id = cur.fetchone()[0]\n",
    "                    cur.execute('''\n",
    "                        INSERT OR REPLACE INTO Publications\n",
    "                        (paper_id, author_id) VALUES (?, ?)''', (paper_id, author_id))\n",
    "            except Exception as e:\n",
    "                log_file.write('Insert Failed for Paper {}, ({}) --- {}\\n'.format(title, arxiv_id, e))\n",
    "                log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://arxiv.org/list/cs.{}/{}{}?show=2000'\n",
    "# fields = ['AI', 'CC', 'CG', 'CE', 'CL', 'CV', 'CY', 'CR', 'DB', 'DS', 'DL', 'DM', 'DC', 'ET', 'FL', \n",
    "#           'GT', 'GL', 'GR', 'AR', 'HC', \n",
    "# fields = ['IR', 'IT', 'LG', 'LO', \n",
    "# fields = ['MS', 'MA', 'MM', 'NI', 'NE', \n",
    "#           'NA', 'OS', 'OH', 'PF', 'PL', 'RO' ,'SI', 'SE', 'SD', 'SC', 'SY']\n",
    "fields = ['LO']\n",
    "months = ['{:0>2d}'.format(i+1) for i in range(12)]\n",
    "years = ['{:0>2d}'.format(i) for i in range(94, 95)]\n",
    "\n",
    "conn = sqlite3.connect('arxiv_raw.sqlite')\n",
    "cur = conn.cursor()\n",
    "cur.executescript('''\n",
    "CREATE TABLE IF NOT EXISTS Papers (\n",
    "    id TEXT NOT NULL PRIMARY KEY,\n",
    "    url TEXT UNIQUE,\n",
    "    title TEXT UNIQUE,\n",
    "    year INTEGER,\n",
    "    month INTEGER,\n",
    "    category TEXT,\n",
    "    primary_subject TEXT,\n",
    "    secondary_subject TEXT,\n",
    "    tertiary_subject TEXT\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS Authors (\n",
    "    id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    name TEXT\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS Publications (\n",
    "    paper_id TEXT, \n",
    "    author_id INTEGER,\n",
    "    PRIMARY KEY (paper_id, author_id)\n",
    ");\n",
    "''')\n",
    "\n",
    "log_file = open('failed_records.txt', 'w')\n",
    "\n",
    "for field in fields:\n",
    "    print('Starting Field {}...'.format(field))\n",
    "    for year in years:\n",
    "        print('Retrieved new IP: {}'.format(get_new_ip(tor_password)))\n",
    "        for month in months:\n",
    "#             sleep(2*np.random.random())\n",
    "            query_url = url.format(field, year, month)\n",
    "            print('Retrieving {}'.format(query_url))\n",
    "            while True:\n",
    "                try:                  \n",
    "                    data = request(query_url).text\n",
    "                    soup = BeautifulSoup(str(data))\n",
    "                    get_papers_metadata(soup, cur, field, year, month, log_file)\n",
    "                    conn.commit()\n",
    "                    break\n",
    "                except:\n",
    "                    print('Request failed, got new IP {}'.format(get_new_ip(tor_password)))\n",
    "                    continue\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Do something.\")\n",
    "    parser.add_argument('-p', '--pdf_dir', required=False)\n",
    "    parser.add_argument('-t', '--text_dir', required=False)\n",
    "    parser.add_argument('-c', '--citation_dir', required=False)\n",
    "    parser.add_argument('-P', '--tor_password', required=True)\n",
    "    parser.add_argument('-f', '--field', required=True)\n",
    "    parser.add_argument('-d', '--db_url', required=True)\n",
    "    parser.add_argument('-b', '--batch_size', type=int, required=True)\n",
    "    parser.add_argument('-s', '--start_index', type=int, required=True)\n",
    "    parser.add_argument('-e', '--end_index', type=int, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    papers = get_ids_from_db(args.db_url, args.field)\n",
    "    print('Found {} papers for category {}'.format(len(papers), args.field))\n",
    "    get_papers_text(papers, batch_size=args.batch_size, citation_dir=args.citation_dir, field=args.field,\n",
    "                    pdf_dir=args.pdf_dir, text_dir=args.text_dir, start_index=args.start_index, end_index=args.end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Do something.\")\n",
    "    parser.add_argument('-p', '--pdf_dir', required=False)\n",
    "    parser.add_argument('-t', '--text_dir', required=False)\n",
    "    parser.add_argument('-c', '--citation_dir', required=False)\n",
    "    parser.add_argument('-P', '--tor_password', required=True)\n",
    "    parser.add_argument('-f', '--field', required=True)\n",
    "    parser.add_argument('-d', '--db_url', required=True)\n",
    "    parser.add_argument('-b', '--batch_size', type=int, required=True)\n",
    "    parser.add_argument('-s', '--start_index', type=int, required=True)\n",
    "    parser.add_argument('-e', '--end_index', type=int, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    papers = get_ids_from_db(args.db_url, args.field)\n",
    "    print('Found {} papers for category {}'.format(len(papers), args.field))\n",
    "    get_papers_text(papers, batch_size=args.batch_size, citation_dir=args.citation_dir, field=args.field,\n",
    "                    pdf_dir=args.pdf_dir, text_dir=args.text_dir, start_index=args.start_index, end_index=args.end_index)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate/Add Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DB Connections\n",
    "from sqlalchemy import create_engine\n",
    "DB_URL = 'arxiv_raw.sqlite'\n",
    "sqlite_conn = sqlite3.connect(DB_URL)\n",
    "sqlalchemy_conn = create_engine('sqlite:///{}'.format(DB_URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citations_root = 'remote_data/'\n",
    "citations_filename = 'citations/citations.txt'\n",
    "for i in range(1, 21):\n",
    "    print(\"Working on set {}\".format(i))\n",
    "    with open(\"{}{}/{}\".format(citations_root, i, citations_filename)) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n', '')\n",
    "            fields = line.split(',')\n",
    "            insert = \"INSERT INTO Citations VALUES ('{}', '{}', {});\".format(fields[0], fields[1], fields[2])\n",
    "#             print(insert)\n",
    "            try: \n",
    "                sqlite_conn.execute(insert)\n",
    "                sqlite_conn.commit()\n",
    "            except:\n",
    "#                 print(\"Duplicate ID:\" + str(fields))\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Scholar Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scholar' from '/Users/paulburkard/Projects/ArXiv/scholar.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(scholar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "querier = scholar.ScholarQuerier()\n",
    "settings = scholar.ScholarSettings()\n",
    "query = scholar.SearchScholarQuery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query.set_include_citations(False)\n",
    "query.set_include_patents(False)\n",
    "query.set_num_page_results(1)\n",
    "query.set_phrase('Safe cooperative robot dynamics on graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "querier.send_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-41407aade4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquerier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "article = querier.articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_id': ['8835157138450230859', 'Cluster ID', 5],\n",
       " 'excerpt': ['This paper introduces the use of vector fields to design, optimize, and implement reactive schedules for safe cooperative robot patterns on planar graphs. We consider automated guided vehicles (AGVs) operating upon a predefined network of pathways. In contrast to the case of locally Euclidean configuration spaces, regularization of collisions is no longer a local procedure, and issues concerning the global topology of configuration spaces must  ...',\n",
       "  'Excerpt',\n",
       "  10],\n",
       " 'num_citations': [42, 'Citations', 3],\n",
       " 'num_versions': [26, 'Versions', 4],\n",
       " 'title': ['Safe cooperative robot dynamics on graphs', 'Title', 0],\n",
       " 'url': ['http://epubs.siam.org/doi/abs/10.1137/S0363012900368442', 'URL', 1],\n",
       " 'url_citation': [None, 'Citation link', 9],\n",
       " 'url_citations': ['http://scholar.google.com/scholar?cites=8835157138450230859&as_sdt=2005&sciodt=1,5&hl=en',\n",
       "  'Citations list',\n",
       "  7],\n",
       " 'url_pdf': [None, 'PDF link', 6],\n",
       " 'url_versions': ['http://scholar.google.com/scholar?cluster=8835157138450230859&hl=en&as_sdt=1,5&as_vis=1',\n",
       "  'Versions list',\n",
       "  8],\n",
       " 'year': ['2002', 'Year', 2]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(article.attrs['url_citations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citations_count = int(soup.find(id='gs_ab_md').text.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Coordinating multiple robots with kinodynamic constraints along specified paths\n",
      "Link: http://journals.sagepub.com/doi/abs/10.1177/0278364905051974\n",
      "0\n",
      "Title: Configuration spaces and braid groups on graphs in robotics\n",
      "Link: https://arxiv.org/abs/math/9905023\n",
      "0\n",
      "Title: M*: A complete multirobot path planning algorithm with performance bounds\n",
      "Link: http://ieeexplore.ieee.org/abstract/document/6095022/\n",
      "0\n",
      "Title: Finding topology in a factory: configuration spaces\n",
      "Link: http://www.jstor.org/stable/2695326\n",
      "0\n",
      "Title: State complexes for metamorphic robots\n",
      "Link: http://journals.sagepub.com/doi/abs/10.1177/0278364904045468\n",
      "1\n",
      "Title: Algorithms for collision-free navigation of mobile robots in complex cluttered environments: a survey\n",
      "Link: https://www.cambridge.org/core/journals/robotica/article/algorithms-for-collision-free-navigation-of-mobile-robots-in-complex-cluttered-environments-a-survey/ADA8F6F7E30123629A26B08DA0C79C8C\n",
      "0\n",
      "Title: Discrete Morse theory and graph braid groups\n",
      "Link: http://msp.org/agt/2005/5-3/p09.xhtml\n",
      "0\n",
      "Title: Topology of robot motion planning\n",
      "Link: http://www.springerlink.com/index/N103R43G77R262N4.pdf\n",
      "0\n",
      "Title: The geometry and topology of reconfiguration\n",
      "Link: http://www.sciencedirect.com/science/article/pii/S0196885806001175\n",
      "0\n",
      "Title: Configuration spaces, braids, and robotics\n",
      "Link: http://books.google.com/books?hl=en&lr=&id=SDPHgcHnrBYC&oi=fnd&pg=PA263&ots=YbHTIvrpuk&sig=glAnbxLEdq8ybxeryuMPBnW1wtc\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "citations = soup.find_all(attrs={'class': 'gs_rt'})\n",
    "for citation in citations:\n",
    "    link = citation.find('a')\n",
    "    count = sqlite_conn.execute(\"SELECT COUNT(*) FROM Papers WHERE title='{}'\".format(link.text)).fetchone()[0]\n",
    "    print(\"Title: {}\".format(link.text))\n",
    "    print(\"Link: {}\".format(link['href']))\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_check = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
